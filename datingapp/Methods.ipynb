{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Don't Go Breaking my Heart: Bayesian/GMM clustering of potential romantic partners\n",
    "\n",
    "The following project is a showcase for how to use a Bayesian evidential approach to BERT embeddings coupled with GMM to generate related clusters of individuals based on their encoded cultural views in language.\n",
    "\n",
    "#### Basic steps\n",
    "1. restrict data to female, gay, and in Berekeley\n",
    "2. Select our target person at random, and a person whom they had a bad experience with, at random.\n",
    "3. Take one of the essays for both people, pull out all NNs in the essay. Use these NNs as $w'$ in other essays.\n",
    "4. Use each NN as a w' to search profiles with\n",
    "5. Generate evidence scores\n",
    "6. Use GMM to cluster close to person and close to bad date.\n",
    "7. Select middle people (because maximal distance from subject, and minimal proximity to bad date.\n",
    "\n",
    "#### Alternative.\n",
    "So this version I'm basing off of the TEDx Talk here: https://www.ted.com/talks/hannah_fry_the_mathematics_of_love/transcript?language=en\n",
    "\n",
    "Here's how we implement her select the 3rd person logic.\n",
    "\n",
    "1. restrict data to female, gay, and in Berekeley\n",
    "2. Select our target person at random, and 3 people whom they had experiences with at random.\n",
    "3. Take one of the essays for all 3 people, pull out all NNs in the essay. Use these NNs as $w'$ in other essays.\n",
    "4. Now, use $P(R|u) \\sim \\sum P(E_{u, w'}|E_{k,w'})$ where u is unknown users, k is the known set of four, and $w'$ are the search terms we pulled from k's profiles.\n",
    "5. Use GMM to cluster the data into 2-groups--yes and no.\n",
    "6. Reduce dimensions for our 4 axes to 2 using PCA.\n",
    "7. Joint kde-plot for visualization.\n",
    "\n",
    "##### Notes:\n",
    "Okay. Pre-sampled is pretty good on Essay 1 and Essay 4. I think I might do essay 1.\n",
    "Search terms I want to include here\n",
    "- 'grad'\n",
    "- 'career'\n",
    "- 'job'\n",
    "- 'travel'\n",
    "- 'human rights'\n",
    "- 'time'\n",
    "\n",
    "If essay 4, just do\n",
    "- 'book'\n",
    "- 'movies'\n",
    "- 'music'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initial samples and data constructor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "df = pd.read_csv('datingapp/data/okcupid_profiles.csv')\n",
    "\n",
    "samples = df.loc[df['orientation'].isin(['gay']) & df['sex'].isin(['f']) & df['location'].isin(['berkeley, california'])].index.tolist()\n",
    "# sampled = np.random.choice(samples, size=(4,), replace=False)\n",
    "sampled = np.array([17148, 49387, 18574,  5060])\n",
    "\n",
    "w_ = ['grad', 'career', 'job', 'human rights', 'time', 'science', 'liberation movement', 'jobs']\n",
    "data = []\n",
    "for i in df.loc[samples].index:\n",
    "    text = str(df['essay1'].loc[i])\n",
    "    data+= [[i,w,text] for w in w_ if w in text.lower()]\n",
    "data = np.array(data)\n",
    "data = pd.DataFrame(data, columns=['id','w','text'])\n",
    "data.to_csv('datingapp/data/population.csv', index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embedded Representations of search terms in context\n",
    "\n",
    "All of this script will be offloaded to the remote server for efficiency (my macbook does not have a great graphics card for pumping out BERT representations/searching through them for the right one). Implementationally, we convert the entirety of a text t to BERT embeddings, and then select only those embeddings indexed by whether or not their tokens are equal to our search term $w'$.\n",
    "\n",
    "$$ E_{t,w'} = BERT(x)\\delta_{w=w'} $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kgen2.BERTcruncher.context_vecs import *\n",
    "\n",
    "PATH = DPATH + 'datingapp/'\n",
    "df = pd.read_csv(PATH + 'population.csv')\n",
    "\n",
    "print(PATH, len(df))\n",
    "\n",
    "level=0\n",
    "\n",
    "#(1) Set up .csv file for data repo\n",
    "vec_data = pd.DataFrame(columns=['id', 'w', 'vec', 'text'])\n",
    "vec_data.to_csv(PATH+'okc-vecs.csv', index=False, encoding='utf-8')\n",
    "\n",
    "#(2) Generate embeddings with appropriate metadata\n",
    "for id,w,TEXT in df.values:\n",
    "    try:\n",
    "        vecs = nC(str(w),str(TEXT),layer_no=level)\n",
    "        update = [[id, str(w), str(vec.view(-1).tolist()), str(TEXT)] for vec in vecs]\n",
    "        update = pd.DataFrame(np.array(update), columns=list(vec_data))\n",
    "        update.to_csv(PATH +'okc-vecs.csv', index=False, encoding='utf-8', header=False, mode='a')\n",
    "    except ValueError:\n",
    "        0\n",
    "    except IndexError:\n",
    "        0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evidentiality Score\n",
    "\n",
    "At the heart of what I'm doing with this project is asking how similar are potential new dates to older dates who the fictional protagonist has dated. To do this, we'll rely on what's basically the weighted evidence that two people are related to one another/part of the same group. This looks like this mathematically, where $j$ is the $j^{th}$ previously dated person, and $u$ is the $u^{th}$ new, undated user of OKC:\n",
    "\n",
    "$$ P(j|u) = \\frac{1}{k_{u}} \\sum_{i,j} \\frac{P(E_{u,w'_i}|E_{j_n,w'_i})}{P(j_n,w'_i)} \\delta_{j_n \\in j} $$\n",
    "\n",
    "Let me explain here. For each term $w'_i$ for each entity in our set of known, previous dates $j$, we'll normalize. Then we'll sum across all of these normalized instances, and renormalize across $u$ to get a probability that u and j are similar."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datingapp.mod.sim_matrix import *\n",
    "\n",
    "df = pd.read_csv('datingapp/data/okc-vecs.csv')\n",
    "\n",
    "sampled = np.array([17148, 49387, 18574,  5060])\n",
    "\n",
    "#We'll use these here to help us use the sel() function\n",
    "# to pick correct folumns and rows as needed.\n",
    "sampled_df = df.loc[df['id'].isin(sampled)]\n",
    "non_sampled_df = df.loc[~df['id'].isin(sampled)]\n",
    "\n",
    "V = torch.FloatTensor([[np.float(i) for i in vec.replace('[', '').replace(']', '').split(', ')] for vec in df['vec'].values])\n",
    "\n",
    "pi = probFn(.1)\n",
    "P = pi.PROB(V[~sel(sampled,df['id'].values)], V[sel(sampled,df['id'].values)])\n",
    "\n",
    "#Creates a 2-D mask of shape (w' x Elements)\n",
    "mW1 = torch.cat([torch.FloatTensor(sel([w_], non_sampled_df['w'].values)).view(1,-1) for w_ in df['w'].unique()])\n",
    "mW2 = torch.cat([torch.FloatTensor(sel([w_], sampled_df['w'].values)).view(1,-1) for w_ in df['w'].unique()])\n",
    "\n",
    "#The objective with these masks is to limit our analysis\n",
    "# to only those instances in which we're comparing\n",
    "# our search term w'i to w'i from our data. This is\n",
    "# just an efficient, linear algebra way of doing this.\n",
    "P = (P * mW1.unsqueeze(-1))\n",
    "P = P.sum(dim=0)\n",
    "P = (P * mW2.unsqueeze(1))\n",
    "P = P.sum(dim=0)\n",
    "P = P/P.sum(dim=0)\n",
    "\n",
    "P = torch.cat([P[:,sel([j], sampled_df['id'].values)].sum(dim=-1).view(-1,1) for j in sampled], dim=-1)\n",
    "P = torch.cat([P[sel([u], non_sampled_df['id'].values)].sum(dim=0).view(1,-1) for u in non_sampled_df['id'].unique()], dim=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This will give use the evidence in favor of two people being similar to one another based on the semantic contexts surrounding key interests that our protagonist is interested in. Now, we want to find from all the remaining people who is a possible, good partner. To do this, we'll use a GMM model to create three clusters. We'll pick people from the cluster that neither includes the bad date or the protagonist--we want some fun differences between us and our partners after all :)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datingapp.mod.gmm_alt import *\n",
    "\n",
    "GMM = GaussianMixture(n_components=3, n_features=4)\n",
    "GMM.fit(P)\n",
    "labels = GMM.predict(P)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can from her reduce the dimensionality of the data (using PCA) and plot it using normal means."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-793193c31cc3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mpca\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPCA\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_components\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mpca\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mP\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mdfi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'P' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(P.numpy())\n",
    "\n",
    "dfi = pd.DataFrame()\n",
    "dfi['country'] = non_sampled_df['id'].unique()\n",
    "dfi[['x0', 'x1']] = pca.transform(P.numpy()) * 4\n",
    "dfi['l'] = labels.view(-1).numpy()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.kdeplot(data=dfi, x='x0', y='x1', hue='l')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}